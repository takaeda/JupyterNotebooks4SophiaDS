{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c0211-2e54-478d-b84e-883a1859d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install scipy==1.12\n",
    "!pip install janome  # janomeをインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e340d92-60b8-40d0-bb3b-22baac62a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# 日本語テキストのトークン化を行うTokenizerの設定\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"テキストをトークン化し、意味を持つ単語（名詞、動詞、形容詞）のみを抽出してリストとして返します。\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token.surface for token in tokens if token.part_of_speech.split(',')[0] in ['名詞', '動詞', '形容詞']]\n",
    "\n",
    "# サンプルデータセットの定義\n",
    "documents = [\n",
    "    \"ディープラーニングは人工知能の一機能です\",\n",
    "    \"人工知能と機械学習はコンピュータサイエンスの一部です\",\n",
    "    \"ディープラーニングはほとんどのAIアプリケーションで使用されています\"\n",
    "]\n",
    "\n",
    "tagged_data = [\n",
    "    TaggedDocument(words=tokenize(doc), tags=[str(i)])\n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "\n",
    "# Doc2Vecモデルのインスタンス作成と設定\n",
    "model = Doc2Vec(vector_size=20, min_count=1, epochs=10)\n",
    "\n",
    "# モデルの語彙を構築\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# モデルの訓練\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# 新しいクエリ文書をトークン化\n",
    "query = \"AIとディープラーニング\"\n",
    "query_tokens = tokenize(query)\n",
    "\n",
    "# クエリ文書のベクトルを推論\n",
    "query_vector = model.infer_vector(query_tokens)\n",
    "\n",
    "# クエリと類似した文書を検索\n",
    "similar_docs = model.dv.most_similar(positive=[query_vector], topn=len(documents))\n",
    "\n",
    "#===============================================\n",
    "# 出力\n",
    "for i, doc in enumerate(documents):\n",
    "    tokens = tokenize(doc)\n",
    "    print(f\"対象文書{i}: {doc}\")\n",
    "    print(f\"\\t分かち書きの結果（トークン化） => {tokens}\\n\")\n",
    "\n",
    "print(f\"\\nクエリ文書: {query}\")\n",
    "query_tokens = tokenize(query)\n",
    "print(f\"\\t分かち書きの結果（トークン化） => {query_tokens}\\n\")\n",
    "\n",
    "print(\"==============================================================\")\n",
    "# 訓練された文書ベクトルを表示\n",
    "print(\"訓練された文書ベクトル:\")\n",
    "for i in range(len(tagged_data)):\n",
    "    print(f\"Document {i}: Vector: {model.dv[str(i)]}\")\n",
    "\n",
    "print(\"\\n推論されたクエリベクトル:\")\n",
    "print(f\"Query Vector: {query_vector}\")\n",
    "\n",
    "# 類似した文書を検索した結果\n",
    "print(\"\\n類似文書とその類似度:\")\n",
    "for doc_id, similarity in similar_docs:\n",
    "    print(f\"文書 {doc_id}: {documents[int(doc_id)]}, 類似度: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
